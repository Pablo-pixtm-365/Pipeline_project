<h1>Pipeline Project</h1>

<h2> Introduction</h2>

There exist many ways to work with data, from small data and big data.<br>
There is not an specific tool that we can use to work with and many approaches and methods to deal with. <br>
In this case, as a part of our Schoolar Project and course we saw many tools, and many methods. We decided to work with Spark, Mongo, Python and Bokeh. <br>
The project is divided in many past, starting from Data Adquisition, pre processing, built of the model, prediction and visualitation. <br>
The objetive of this project is to answer the following questions: 
* What are the most likely places where an earthquake could occur in 2017?
* Is there a big difference in the number of earthquakes that have occurred during the years from 1965 to 2016?
* Are there any incremental changes in the average magnitude of earthquakes over the years?
<h2>Justification</h2>
<h3> Pipeline</h3>

* Data Source: 
  * We took the useful earthquakes'data set (1965-2016) from **Kaggle**. <br>
  We used this platform as our way to get the data, nevertheless, the original data belongs to *The National Earthquake Information Center* 
  
* Data Acquisition:
 * Even when we obtained the data from *kaggle* it's important to know that this type of data is usually obtained by analog sensors that take samples of signals that come from the outside world, such as time, temperature, sound, light (they do not have a fixed value). These sensors are connected to a device that does the conversion of analog values to computational values (with which we can work).<br>
However, the precise moment of obtaining data from us would be from our device, with which we download and begin the analysis of the previously mentioned dataset.

* Data Cleaning


* Data Storage
